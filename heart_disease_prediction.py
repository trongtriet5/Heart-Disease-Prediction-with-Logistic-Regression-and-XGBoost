# -*- coding: utf-8 -*-
"""heart-disease-prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h3PxxZ1OLzunHO5Mqho05DZuK8kSFkg6
"""

!pip install eli5
!pip install category_encoders
!pip install xgboost

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import seaborn as sns
import eli5
from eli5.sklearn import PermutationImportance
from sklearn import metrics
import category_encoders as ce

# Load dataset
df = pd.read_csv('./heart.csv')

# Display first 10 rows with a color gradient
df.head(10).style.background_gradient(cmap="Reds")

# Check for missing values
df.isnull().sum()

# Dataset information
df.info()

# Countplot for target variable
sns.countplot(x = df.HeartDisease)

# Normalize value counts of ST_Slope grouped by HeartDisease
df.groupby('HeartDisease')['ST_Slope'].value_counts(normalize = True)

# Pairplot with hue as HeartDisease
sns.pairplot(df,hue = 'HeartDisease', height = 2)

# Identify rows with RestingBP = 0
df[df.RestingBP == 0]

# Drop invalid row
df.drop(index = 449, inplace = True)

# Encode ExerciseAngina
df["ExerciseAngina"] = df["ExerciseAngina"].apply(lambda x: 1 if x=="Y" else 0)

# Encode Sex
df.Sex = df.Sex =='M'
df.rename(columns = {'Sex':'Male'}, inplace = True)

# Target encoding for categorical features
encoder = ce.TargetEncoder()
features = ['ChestPainType', 'RestingECG', 'ST_Slope']
df[features] = encoder.fit_transform(df[features], df['HeartDisease'])

# Correlation matrix for HeartDisease
corr = df.corr().HeartDisease.sort_values(ascending=False)
corr = corr.to_frame()
corr.style.background_gradient(cmap="Reds")

# Histograms of features
df.drop(columns='HeartDisease').hist(figsize=(10,10), grid=True, color='crimson')
plt.tight_layout()

# Define features and target
X = df.drop(columns='HeartDisease')
y = df['HeartDisease']

print("Features = ", [feature for feature in X.columns])
print('')
print("Number of features = ", len(X.columns))

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=67)

# XGBoost Classifier
clf_xgb = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.05, use_label_encoder=False)
clf_xgb.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_test, y_test)], verbose=False)

clf_xgb = xgb.XGBClassifier(n_estimators=clf_xgb.best_iteration, random_state=73, use_label_encoder=False)
clf_xgb.fit(X_train, y_train)

# Feature importance for XGBoost
feature_imp = pd.Series(clf_xgb.feature_importances_, index=X.columns).sort_values(ascending=False)
print(feature_imp)

sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Feature Importances")
plt.legend()
plt.show()

# Permutation importance for XGBoost
perm = PermutationImportance(clf_xgb, random_state=1).fit(X_train, y_train)
eli5.show_weights(perm, feature_names=X_train.columns.tolist())

y_pred_proba_xgb = clf_xgb.predict_proba(X_test)[:, 1]
fpr_xgb, tpr_xgb, _ = metrics.roc_curve(y_test, y_pred_proba_xgb)
auc_xgb = metrics.roc_auc_score(y_test, y_pred_proba_xgb)

# roc curve with AUC score
plt.plot(fpr_xgb, tpr_xgb, label="XGBoost AUC=" + str(auc_xgb))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()

# Logistic Regression
clf_lr = LogisticRegression(max_iter=1000)
clf_lr.fit(X_train, y_train)

# ROC Curve and AUC for Logistic Regression
y_pred_proba_lr = clf_lr.predict_proba(X_test)[:, 1]
fpr_lr, tpr_lr, _ = metrics.roc_curve(y_test, y_pred_proba_lr)
auc_lr = metrics.roc_auc_score(y_test, y_pred_proba_lr)

plt.plot(fpr_lr, tpr_lr, label="Logistic Regression AUC=" + str(auc_lr))

plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()

# Accuracy scores for both models
print("XGBoost Classifier Accuracy:", clf_xgb.score(X_test, y_test).round(2))
print("Logistic Regression Accuracy:", clf_lr.score(X_test, y_test).round(2))